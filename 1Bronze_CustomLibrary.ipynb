{"cells":[{"cell_type":"code","execution_count":null,"id":"cdbbd571-6dab-49b9-b4b9-0ff926778e69","metadata":{},"outputs":[],"source":["from notebookutils import mssparkutils\n","import multiprocessing\n","from multiprocessing.pool import ThreadPool\n","from ast import literal_eval\n","from pyspark.sql.types import *\n","import json\n","import pytz\n","import datetime\n","import requests\n","import hashlib\n","import hmac\n","import base64"]},{"cell_type":"code","execution_count":null,"id":"c83a9385-619e-4cdc-9d3e-43260ddd9d0d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# set up variable for log analytics\n","## Retrive tenant id, client id and client secret from key vault\n","key_vault = \"https://xxxxxxx.vault.azure.net/\"\n","\n","# Get customer id and share key of log analytic\n","customer_id =  mssparkutils.credentials.getSecret(key_vault, 'LogAnalyticCustomerID')\n","shared_key = mssparkutils.credentials.getSecret(key_vault, 'LogAnalyticsShareKey')\n","\n","# Set log table name\n","log_type = 'FabricActivityLog'"]},{"cell_type":"code","execution_count":null,"id":"9ef96892-91ed-4580-a5dd-e4ce531f2c51","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# set datetime\n","autz = pytz.timezone('Australia/Melbourne')\n","# dateformat = '%d/%m/%Y %H:%M:%S'\n","dateformat = '%m/%d/%Y %H:%M:%S'\n","autodaydate =  datetime.datetime.now().astimezone(autz)"]},{"cell_type":"code","execution_count":null,"id":"978e7a36-172d-4076-9cd3-239f27e4fca2","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["## Schema of log table\n","log_schema =  StructType([\n","    StructField(\"WorkspaceName\", StringType()),\n","    StructField(\"NotebookName\", StringType()),\n","    StructField(\"LakehouseName\", StringType()),\n","    StructField(\"TableName\", StringType()),\n","    StructField(\"LoadType\", StringType()),\n","    StructField(\"Environment\", StringType()),\n","    StructField(\"Status\", StringType()),\n","    StructField(\"StartTime\", StringType()),\n","    StructField(\"EndTime\", StringType()),\n","    StructField(\"DurationSec\", DoubleType()),\n","    StructField(\"RowWritten\", IntegerType()), \n","    StructField(\"StatusMessage\", StringType())\n","])"]},{"cell_type":"code","execution_count":null,"id":"fb08346f-6459-49c0-ba10-612ec3a0a3bb","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#######################################\n","######Log Analytics Functions##########  \n","#######################################\n","\n","# Build the API signature\n","def build_signature(customer_id, shared_key, date, content_length, method, content_type, resource):\n","    x_headers = 'x-ms-date:' + date\n","    string_to_hash = method + \"\\n\" + str(content_length) + \"\\n\" + content_type + \"\\n\" + x_headers + \"\\n\" + resource\n","    bytes_to_hash = bytes(string_to_hash, encoding=\"utf-8\")  \n","    decoded_key = base64.b64decode(shared_key)\n","    encoded_hash = base64.b64encode(hmac.new(decoded_key, bytes_to_hash, digestmod=hashlib.sha256).digest()).decode()\n","    authorization = \"SharedKey {}:{}\".format(customer_id,encoded_hash)\n","    return authorization\n","\n","# Build and send a request to the POST API\n","def post_data(customer_id, shared_key, body, log_type):\n","    method = 'POST'\n","    content_type = 'application/json'\n","    resource = '/api/logs'\n","    rfc1123date = datetime.datetime.utcnow().strftime('%a, %d %b %Y %H:%M:%S GMT')\n","\n","    content_length = len(body)\n","    signature = build_signature(customer_id, shared_key, rfc1123date, content_length, method, content_type, resource)\n","    uri = 'https://' + customer_id + '.ods.opinsights.azure.com' + resource + '?api-version=2016-04-01'\n","\n","    headers = {\n","        'content-type': content_type,\n","        'Authorization': signature,\n","        'Log-Type': log_type,\n","        'x-ms-date': rfc1123date\n","    }\n","\n","    response = requests.post(uri,data=body, headers=headers)\n","    if (response.status_code >= 200 and response.status_code <= 299):\n","        print('Accepted')\n","    else:\n","        print(\"Response code: {}\".format(response.status_code))"]},{"cell_type":"code","execution_count":null,"id":"77aba208-4a4e-447f-85fc-b9fc63181678","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["## Run notebook function\n","def run_notebook(notebook_info):\n","        table_name = notebook_info['TableName']\n","        notebook_name = childnotebook\n","        loadtype = notebook_info['LoadType']\n","        businesskey = notebook_info['BusinessKeys']\n","        watermarkcolumn = notebook_info['WatermarkColumn']\n","        start_time = datetime.datetime.now().astimezone(autz)\n","\n","        # try run child notebook\n","        try:\n","                nboutput = mssparkutils.notebook.run(notebook_name, timeout_seconds= 60*60\\\n","                        , arguments = {'shortcutname': shortcutname , 'lakehousename': lakehousename, 'tablename': table_name , 'loadtype': loadtype\\\n","                                        , 'businesskey': businesskey, 'watermarkcolumn': watermarkcolumn} )\n","                end_time = datetime.datetime.now().astimezone(autz)\n","                duration = (end_time - start_time).total_seconds()\n","                nboutput = json.loads(nboutput)\n","                rows_written, nbresult = nboutput['rowswritten'], nboutput['nbresult']\n","                data = [{'WorkspaceName': workspacename, \"NotebookName\": notebook_name,'LakehouseName': lakehousename\n","                        ,  \"TableName\" : table_name, \"LoadType\" : loadtype , \"Environment\": f\"Dev\", \"Status\": f\"Success\"\n","                        , \"StartTime\": start_time.strftime(dateformat), \"EndTime\": end_time.strftime(dateformat)\n","                        , \"DurationSec\":duration, \"RowWritten\": rows_written, \"StatusMessage\": nbresult}]\n","                print('success - ' + table_name)\n","\n","        # fail to run, collect error message\n","        except Exception as e:\n","                error_message = \"Error Message - \" + str(e)\n","                end_time = datetime.datetime.now().astimezone(autz).strftime(dateformat)\n","                duration = (end_time - start_time).total_seconds()\n","                rows_written = 0\n","                data = [{'WorkspaceName': workspacename, \"NotebookName\": notebook_name, 'LakehouseName': lakehousename\n","                        , \"TableName\" : table_name, \"LoadType\" : loadtype , \"Environment\": f\"Dev\",  \"Status\": f\"Failure\"\n","                        , \"StartTime\": start_time.strftime(dateformat), \"EndTime\": end_time.strftime(dateformat)\n","                        ,\"DurationSec\":duration, \"RowWritten\": rows_written, \"StatusMessage\":{error_message}}]\n","                print('fail - ' + table_name)\n","        \n","        #save log data into fabric table and log analytic workspace\n","        new_log_row = spark.createDataFrame(data, schema=log_schema)\n","        new_log_row.write.format(\"delta\").mode(\"append\").saveAsTable(fabriclogtable)\n","        log_body = json.dumps(data)\n","        post_data(customer_id, shared_key, log_body, log_type)"]}],"metadata":{"language_info":{"name":"python"},"microsoft":{"host":{},"language":"python"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
