{"cells":[{"cell_type":"code","execution_count":5,"id":"b7bb0ad5-3e24-47f0-ba90-a124db0e3d93","metadata":{},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-12-11T21:57:37.0647676Z","execution_start_time":"2023-12-11T21:57:36.5880761Z","livy_statement_state":"available","parent_msg_id":"9378aa2b-9ceb-4174-85e9-7af5ef0598c0","queued_time":"2023-12-11T21:57:36.316606Z","session_id":"5ee512e7-a812-4c87-8edd-1c078edd66d6","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":7},"text/plain":["StatementMeta(, 5ee512e7-a812-4c87-8edd-1c078edd66d6, 7, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["## library and datetime variable\n","from notebookutils import mssparkutils\n","import pandas as pd\n","from datetime import datetime\n","import pytz\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import *\n","from delta.tables import *\n","import json\n","# import pandas\n","# pd.set_option('display.max.columns', None)\n","\n","autz = pytz.timezone('Australia/Melbourne')\n","dateformat = '%Y%m%d%H%M%S'\n","todaydatetime = datetime.now().astimezone(autz).strftime(dateformat)"]},{"cell_type":"code","execution_count":6,"id":"259d7aae-55d6-44b3-b07e-03d82f0f5549","metadata":{},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-12-11T21:57:37.7181921Z","execution_start_time":"2023-12-11T21:57:37.3723808Z","livy_statement_state":"available","parent_msg_id":"bea6c1ec-73a8-45b1-88ca-8276ebb026bd","queued_time":"2023-12-11T21:57:36.5078564Z","session_id":"5ee512e7-a812-4c87-8edd-1c078edd66d6","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":8},"text/plain":["StatementMeta(, 5ee512e7-a812-4c87-8edd-1c078edd66d6, 8, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# # ## print variables from parent notebook\n","# watermarkcolumn = \"columnname1\"\n","# businesskey = \"columnname2\"\n","# lakehousename = \"lakehousename\"\n","# shortcutname = \"shortcutfoldername\"\n","# loadtype = \"incremental\"\n","# tablename = \"tablename\""]},{"cell_type":"code","execution_count":7,"id":"95b9b4b5-222d-4322-86f9-f0d4434c1355","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-12-11T21:57:38.3579314Z","execution_start_time":"2023-12-11T21:57:38.0654674Z","livy_statement_state":"available","parent_msg_id":"377ed81a-2a43-4d7e-93dc-f55865fc8ef6","queued_time":"2023-12-11T21:57:36.7406719Z","session_id":"5ee512e7-a812-4c87-8edd-1c078edd66d6","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":9},"text/plain":["StatementMeta(, 5ee512e7-a812-4c87-8edd-1c078edd66d6, 9, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["## Create a archive folder in table directory\n","tabledirectory = 'Files/' + shortcutname + '/' + tablename\n","archivefolder = 'Files/' + shortcutname + '/' + tablename + '/archive/' \n","numberarchivefiles = len([i for i in mssparkutils.fs.ls(tabledirectory) if 'archive' == i.name])\n","if numberarchivefiles == 0 : mssparkutils.fs.mkdirs(archivefolder)\n","else: pass\n","# print(archivefolder)"]},{"cell_type":"code","execution_count":24,"id":"49e8c353-e682-48d5-91f7-8961739f6c1e","metadata":{},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-12-11T22:10:50.9556477Z","execution_start_time":"2023-12-11T22:10:42.6709856Z","livy_statement_state":"available","parent_msg_id":"349a7ac7-6c8f-4d38-824a-3c2ee6ec1db9","queued_time":"2023-12-11T22:10:42.4040661Z","session_id":"5ee512e7-a812-4c87-8edd-1c078edd66d6","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-12-11T22:10:49.043GMT","dataRead":6343,"dataWritten":0,"description":"Delta: Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...: Compute snapshot for version: 2","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","jobGroup":"26","jobId":74,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":53,"numTasks":54,"rowCount":50,"stageIds":[107,105,106],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:49.007GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:48.993GMT","dataRead":11248,"dataWritten":6343,"description":"Delta: Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...: Compute snapshot for version: 2","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","jobGroup":"26","jobId":73,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":61,"stageIds":[103,104],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:48.499GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:48.384GMT","dataRead":31697,"dataWritten":11248,"description":"Delta: Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...: Compute snapshot for version: 2","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","jobGroup":"26","jobId":72,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":22,"stageIds":[102],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:48.329GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:48.159GMT","dataRead":31697,"dataWritten":0,"description":"Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...","displayName":"toString at String.java:2951","jobGroup":"26","jobId":71,"killedTasksSummary":{},"name":"toString at String.java:2951","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":11,"stageIds":[101],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:48.115GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:46.780GMT","dataRead":1295,"dataWritten":83601,"description":"Delta: Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...: Writing merged data full update","displayName":"run at ForkJoinTask.java:1426","jobGroup":"26","jobId":70,"killedTasksSummary":{},"name":"run at ForkJoinTask.java:1426","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":3,"numSkippedTasks":3,"numTasks":4,"rowCount":4,"stageIds":[99,100,97,98],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:46.511GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:46.466GMT","dataRead":2435,"dataWritten":1295,"description":"Delta: Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...: Writing merged data full update","displayName":"run at ForkJoinTask.java:1426","jobGroup":"26","jobId":69,"killedTasksSummary":{},"name":"run at ForkJoinTask.java:1426","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":2,"numTasks":3,"rowCount":5,"stageIds":[96,94,95],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:46.056GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:46.013GMT","dataRead":57939,"dataWritten":1152,"description":"Delta: Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...: Writing merged data full update","displayName":"run at ForkJoinTask.java:1426","jobGroup":"26","jobId":68,"killedTasksSummary":{},"name":"run at ForkJoinTask.java:1426","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":4,"stageIds":[93],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:45.818GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:45.808GMT","dataRead":1286,"dataWritten":1283,"description":"Delta: Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...: Writing merged data full update","displayName":"run at ForkJoinTask.java:1426","jobGroup":"26","jobId":67,"killedTasksSummary":{},"name":"run at ForkJoinTask.java:1426","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":2,"stageIds":[92],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:45.747GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:45.997GMT","dataRead":79699,"dataWritten":21764,"description":"Delta: Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...: Writing merged no shuffle data","displayName":"run at ForkJoinTask.java:1426","jobGroup":"26","jobId":66,"killedTasksSummary":{},"name":"run at ForkJoinTask.java:1426","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":2,"stageIds":[91],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:45.542GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:45.184GMT","dataRead":491,"dataWritten":0,"description":"Delta: Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...: Finding touched files - low shuffle merge","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","jobGroup":"26","jobId":65,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":200,"numCompletedStages":1,"numCompletedTasks":200,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":202,"rowCount":1,"stageIds":[89,90],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:44.504GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:44.456GMT","dataRead":12270181,"dataWritten":491,"description":"Delta: Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...: Finding touched files - low shuffle merge","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","jobGroup":"26","jobId":64,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":1059138,"stageIds":[88],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:44.028GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:43.743GMT","dataRead":5617,"dataWritten":0,"description":"Delta: Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...: Finding touched files - low shuffle merge","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","jobGroup":"26","jobId":62,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":5,"stageIds":[85,86],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:43.632GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:43.533GMT","dataRead":48380,"dataWritten":0,"description":"Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","jobGroup":"26","jobId":61,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":1,"stageIds":[84],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:43.411GMT","usageDescription":""},{"completionTime":"2023-12-11T22:10:43.211GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 26:\n## Get filename from directory\nfilepath = 'Files/' + shortcutname + '/' + tablename + '/'\nfilelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\nfilelist.sort()\ntablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n\n## Read file and save into table\n## full load\nif loadtype == 'full':\n    if len(filelist)>0:\n\n        # read and write file\n        df = spark.read.parquet(filepath)\n        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n\n        ## move file into archive\n        currpath = filelist[0]\n        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n        print('full load - completed, overwrite')\n\n    else: print('full load - completed, no new file')\n\n\n# ## incremental load\nelif loadtype == 'incremental':\n    if len(t...","displayName":"load at NativeMethodAccessorImpl.java:0","jobGroup":"26","jobId":60,"killedTasksSummary":{},"name":"load at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[83],"status":"SUCCEEDED","submissionTime":"2023-12-11T22:10:42.855GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":14,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":26},"text/plain":["StatementMeta(, 5ee512e7-a812-4c87-8edd-1c078edd66d6, 26, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["incremental load - upsert rows\n","from file - Filteredupbeat_transaction_daily_20231208010001.parquet\n"]}],"source":["## Get filename from directory\n","filepath = 'Files/' + shortcutname + '/' + tablename + '/'\n","filelist = [filepath + i.name for i in mssparkutils.fs.ls(filepath) if 'archive' not in i.name and '_delta_log' not in i.name ]\n","filelist.sort()\n","tablelist = [i.name for i in mssparkutils.fs.ls('Tables/') if tablename.lower() == i.name]\n","\n","## Read file and save into table\n","## full load\n","if loadtype == 'full':\n","    if len(filelist)>0:\n","\n","        # read and write file\n","        df = spark.read.parquet(filepath)\n","        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n","        numrows = df.count()\n","\n","        ## move file into archive\n","        currpath = filelist[0]\n","        archivepath = filelist[0].replace(filepath, archivefolder).replace('.parquet', '_' + todaydatetime +'.parquet')\n","        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n","        nb_result = 'full load - completed, overwrite'\n","\n","    else: \n","        numrows = 0\n","        nb_result = 'full load - completed, no new file'\n","\n","# ## incremental load\n","elif loadtype == 'incremental':\n","    if len(tablelist)== 0 and len(filelist) > 0 :\n","        nb_result = 'incremental load - write file to a new table'\n","\n","        ## read file and write into fabric table\n","        df = spark.read.parquet(filepath)\n","        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"True\").saveAsTable(tablename)\n","        numrows = df.count()\n","\n","        ## move file into archive folder\n","        currpath = filelist[0]\n","        archivepath = filelist[0].replace(filepath, archivefolder)\n","        mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n","        print('from file - ' + filelist[0].replace(filepath,''))\n","\n","    elif len(filelist) > 0 :\n","        nb_result = 'incremental load - upsert rows'\n","        for i in filelist:\n","\n","            ## read fabric table, new file, then merge them\n","            tablenamelowercase = tablename.lower()\n","            dfold = DeltaTable.forPath(spark, \"Tables/\" + tablename.lower())\n","            dfnew = spark.read.format(\"parquet\").load(filepath)\n","            (dfold.alias(\"o\")\\\n","                .merge(dfnew.alias(\"n\"), f\"o.{businesskey} == n.{businesskey}\")\\\n","                .whenMatchedUpdateAll()\\\n","                .whenNotMatchedInsertAll()\\\n","                .execute() )\n","            numrows = dfnew.count()\n","\n","            ## move file into archive folder\n","            currpath = i\n","            archivepath = i.replace(filepath, archivefolder)\n","            mssparkutils.fs.mv(currpath, archivepath, overwrite=True)\n","            print('from file - ' + i.replace(filepath,''))\n","            \n","    else: \n","        numrows = 0\n","        nb_result = 'incremental load - completed, no new file'\n","\n","else: pass\n","print(nb_result)"]},{"cell_type":"code","execution_count":null,"id":"f0472f18-91d6-40ca-b71a-9a5864f18a92","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["## pass number of rows to the parent notebook\n","mssparkutils.notebook.exit(json.dumps({\"rowswritten\":numrows, \"nbresult\": nb_result}))"]}],"metadata":{"a365ComputeOptions":null,"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"host":{"synapse_widget":{"state":{},"token":"d0d8ec49-13d4-4dbb-ae77-631fd5e3972c"}},"language":"python"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"sessionKeepAliveTimeout":0,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"a223b45f-483d-4d90-93ec-833ec4cf1be1","default_lakehouse_name":"Upbeat_dev","default_lakehouse_workspace_id":"4e190474-6bf4-41ff-8e98-77dbfc7c43f1","known_lakehouses":[{"id":"a223b45f-483d-4d90-93ec-833ec4cf1be1"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
